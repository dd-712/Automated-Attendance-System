{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the dependencies\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./img/Jeff.jpg\")\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "faces = faceCascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    roi_gray = gray_img[y:y+h, x:x+w]\n",
    "    cv2.imwrite(\"./images/Jeff.jpeg\",roi_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding images loaded\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import face_recognition\n",
    "\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "frame_resizing = 1\n",
    "\n",
    "images_path = glob.glob(os.path.join(\"./images/\", \"*.*\"))\n",
    "for img_path in images_path:\n",
    "    img = cv2.imread(img_path)\n",
    "    # rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get the filename only from the initial file path.\n",
    "    basename = os.path.basename(img_path)\n",
    "    (filename, ext) = os.path.splitext(basename)\n",
    "    # Get encoding\n",
    "    img_encoding = face_recognition.face_encodings(img)[0]\n",
    "\n",
    "    # Store file name and file encoding\n",
    "    known_face_encodings.append(img_encoding)\n",
    "    known_face_names.append(filename)\n",
    "print(\"Encoding images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_known_faces(frame):\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=frame_resizing, fy=frame_resizing)\n",
    "    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_face_names[best_match_index]\n",
    "        face_names.append(name)\n",
    "\n",
    "    face_locations = np.array(face_locations)\n",
    "    face_locations = face_locations / frame_resizing\n",
    "    return face_locations.astype(int), face_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    face_locations, face_names = detect_known_faces(gray_img)\n",
    "    for face_loc, name in zip(face_locations, face_names):\n",
    "        y1, x1, y2, x2 = face_loc[0], face_loc[1], face_loc[2], face_loc[3]\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 200), 2)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 200), 4)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial emotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"./haarcascade/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = video.read()\n",
    "\n",
    "    gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "\n",
    "    # hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # h, s, v = cv2.split(hsv)\n",
    "\n",
    "    # lim = 255 - 20\n",
    "    # v[v > lim] = 255\n",
    "    # v[v <= lim] += 20\n",
    "\n",
    "    # final_hsv = cv2.merge((h, s, v))\n",
    "    # frame = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    try:\n",
    "        res = DeepFace.analyze(img_path = frame, actions = [\"emotion\"])['emotion']\n",
    "    \n",
    "        emotion = max(zip(res.values(), res.keys()))[1]\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.putText(frame, emotion, (x+5,y-5), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 200), 4)\n",
    "    except Exception:\n",
    "        print(\"some error\")\n",
    "\n",
    "    cv2.imshow(\"Capturing\", frame)\n",
    "    key=cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "            break\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some error\n",
      "some error\n",
      "some error\n",
      "some error\n",
      "some error\n",
      "some error\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = video.read()\n",
    "\n",
    "    gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    faces = faceCascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "\n",
    "    # res = DeepFace.analyze(img_path = frame, actions = [\"emotion\"])['emotion']\n",
    "    \n",
    "            \n",
    "    for (x, y, w, h) in faces:\n",
    "        \n",
    "        roi_gray = gray_img[y:y+h, x:x+w]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        try:\n",
    "            res = DeepFace.analyze(img_path = gray_img, actions = [\"emotion\"])['emotion']\n",
    "            emotion = max(zip(res.values(), res.keys()))[1]\n",
    "\n",
    "            \n",
    "            cv2.putText(frame, emotion, (x+5,y-5), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "        except Exception:\n",
    "            print(\"some error\")\n",
    "\n",
    "    cv2.imshow(\"Capturing\", frame)\n",
    "    key=cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "            break\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Enhancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def readImagesAndTimes():\n",
    "  \n",
    "  times = np.array([ 1/30.0, 0.25, 2.5, 15.0 ], dtype=np.float32)\n",
    "  \n",
    "  filenames = [\"img_0.033.jpg\", \"img_0.25.jpg\", \"img_2.5.jpg\", \"img_15.jpg\"]\n",
    "\n",
    "  images = []\n",
    "  for filename in filenames:\n",
    "    im = cv2.imread(filename)\n",
    "    images.append(im)\n",
    "  \n",
    "  return images, times\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Read images and exposure times\n",
    "  print(\"Reading images ... \")\n",
    "\n",
    "  images, times = readImagesAndTimes()\n",
    "  \n",
    "  \n",
    "  # Align input images\n",
    "  print(\"Aligning images ... \")\n",
    "  alignMTB = cv2.createAlignMTB()\n",
    "  alignMTB.process(images, images)\n",
    "  \n",
    "  # Obtain Camera Response Function (CRF)\n",
    "  print(\"Calculating Camera Response Function (CRF) ... \")\n",
    "  calibrateDebevec = cv2.createCalibrateDebevec()\n",
    "  responseDebevec = calibrateDebevec.process(images, times)\n",
    "  \n",
    "  # Merge images into an HDR linear image\n",
    "  print(\"Merging images into one HDR image ... \")\n",
    "  mergeDebevec = cv2.createMergeDebevec()\n",
    "  hdrDebevec = mergeDebevec.process(images, times, responseDebevec)\n",
    "  # Save HDR image.\n",
    "  cv2.imwrite(\"hdrDebevec.hdr\", hdrDebevec)\n",
    "  print(\"saved hdrDebevec.hdr \")\n",
    "  \n",
    "  # # Tonemap using Drago's method to obtain 24-bit color image\n",
    "  print(\"Tonemaping using Drago's method ... \")\n",
    "  tonemapDrago = cv2.createTonemapDrago(1.0, 0.7)\n",
    "  ldrDrago = tonemapDrago.process(hdrDebevec)\n",
    "  ldrDrago = 3 * ldrDrago\n",
    "  cv2.imwrite(\"ldr-Drago.jpg\", ldrDrago * 255)\n",
    "  print(\"saved ldr-Drago.jpg\")\n",
    "  \n",
    "  \n",
    "  # # Tonemap using Reinhard's method to obtain 24-bit color image\n",
    "  print(\"Tonemaping using Reinhard's method ... \")\n",
    "  tonemapReinhard = cv2.createTonemapReinhard(1.5, 0,0,0)\n",
    "  ldrReinhard = tonemapReinhard.process(hdrDebevec)\n",
    "  cv2.imwrite(\"ldr-Reinhard.jpg\", ldrReinhard * 255)\n",
    "  print(\"saved ldr-Reinhard.jpg\")\n",
    "  \n",
    "  # # Tonemap using Mantiuk's method to obtain 24-bit color image\n",
    "  print(\"Tonemaping using Mantiuk's method ... \")\n",
    "  tonemapMantiuk = cv2.createTonemapMantiuk(2.2,0.85, 1.2)\n",
    "  ldrMantiuk = tonemapMantiuk.process(hdrDebevec)\n",
    "  ldrMantiuk = 3 * ldrMantiuk\n",
    "  cv2.imwrite(\"ldr-Mantiuk.jpg\", ldrMantiuk * 255)\n",
    "  print(\"saved ldr-Mantiuk.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('minor': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9bb0771ba77142be604c9de6ba6c4b994c8f26247daff9097cd077836cb053a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
